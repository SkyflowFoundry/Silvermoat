name: E2E Tests with Ephemeral Stack

on:
  pull_request:
    branches: [main]
    paths:
      - 'cdk/**'
      - 'lambda/**'
      - 'ui/**'
      - 'tests/**'
      - 'scripts/**'
      - '.github/workflows/**'
  workflow_dispatch:
    inputs:
      pr_number:
        description: 'PR number for stack naming (optional)'
        required: false
      create_issues:
        description: 'Auto-create GitHub issues for test failures'
        type: boolean
        default: true
        required: false
      force_deploy:
        description: 'Force stack deployment even if no changes detected'
        required: false
        default: 'false'
        type: choice
        options:
          - 'false'
          - 'true'

permissions:
  id-token: write
  contents: read
  pull-requests: write
  issues: write

jobs:
  detect-changes:
    name: Detect Changed Files
    runs-on: ubuntu-latest
    outputs:
      infrastructure_changed: ${{ steps.filter.outputs.infrastructure }}
      ui_changed: ${{ steps.filter.outputs.ui }}
      tests_changed: ${{ steps.filter.outputs.tests }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Detect file changes
        uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            infrastructure:
              - 'cdk/**'
              - 'lambda/**'
              - 'scripts/deploy-stack.sh'
              - 'scripts/deploy-ui.sh'
            ui:
              - 'ui/**'
            tests:
              - 'tests/**'
              - '.github/workflows/e2e-tests.yml'

  validate-cdk:
    needs: detect-changes
    if: needs.detect-changes.outputs.infrastructure_changed == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install AWS CDK
        run: npm install -g aws-cdk

      - name: Install CDK dependencies
        run: |
          cd cdk
          pip install -r requirements.txt

      - name: Validate CDK stacks (synth)
        env:
          STACK_NAME: silvermoat-test-pr-123
          STAGE_NAME: test
          CREATE_CLOUDFRONT: "false"
          DOMAIN_NAME: ""
        run: |
          echo "Validating CDK stacks via synthesis..."
          cd cdk
          cdk synth silvermoat-test-pr-123 > /dev/null
          echo "‚úÖ CDK stack synthesis successful"

  unit-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup environment
        uses: ./.github/actions/setup-env

      - name: Install UI dependencies
        uses: ./.github/actions/install-deps
        with:
          install-ui: 'true'

      - name: Run JavaScript unit tests
        run: |
          cd ui
          npm test -- --run

  setup-stack:
    needs: [detect-changes, validate-cdk, unit-tests]
    if: always() && (needs.validate-cdk.result == 'success' || needs.validate-cdk.result == 'skipped')
    runs-on: ubuntu-latest
    timeout-minutes: 20

    outputs:
      stack_name: ${{ steps.stack-name.outputs.stack_name }}
      api_url: ${{ steps.stack-outputs.outputs.api_url }}
      web_url: ${{ steps.stack-outputs.outputs.web_url }}
      skip_deploy: ${{ steps.check-deployment.outputs.skip_deploy || 'false' }}
      skip_reason: ${{ steps.check-deployment.outputs.skip_reason || '' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup environment
        uses: ./.github/actions/setup-env

      - name: Install dependencies
        uses: ./.github/actions/install-deps

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: us-east-1

      - name: Determine test stack name
        id: stack-name
        run: |
          if [ -n "${{ github.event.pull_request.number }}" ]; then
            STACK_NAME="silvermoat-test-pr-${{ github.event.pull_request.number }}"
          elif [ -n "${{ inputs.pr_number }}" ]; then
            STACK_NAME="silvermoat-test-pr-${{ inputs.pr_number }}"
          else
            STACK_NAME="silvermoat-test-${{ github.run_id }}"
          fi
          echo "stack_name=${STACK_NAME}" >> $GITHUB_OUTPUT
          echo "Test stack name: ${STACK_NAME}"

      - name: Check if stack exists
        id: check-stack
        run: |
          STACK_NAME="${{ steps.stack-name.outputs.stack_name }}"
          echo "Checking if stack ${STACK_NAME} exists..."

          # Check stack status
          STACK_STATUS=$(aws cloudformation describe-stacks \
            --stack-name "${STACK_NAME}" \
            --query 'Stacks[0].StackStatus' \
            --output text 2>/dev/null || echo "NOT_FOUND")

          echo "Stack status: ${STACK_STATUS}"
          echo "stack_status=${STACK_STATUS}" >> $GITHUB_OUTPUT

          # Handle failed rollback states - delete stack before redeploying
          if [[ "${STACK_STATUS}" == "ROLLBACK_COMPLETE" ]] || \
             [[ "${STACK_STATUS}" == "ROLLBACK_FAILED" ]] || \
             [[ "${STACK_STATUS}" == "UPDATE_ROLLBACK_COMPLETE" ]] || \
             [[ "${STACK_STATUS}" == "UPDATE_ROLLBACK_FAILED" ]]; then
            echo "üóëÔ∏è Stack in failed state (${STACK_STATUS}), deleting before redeployment..."
            cd cdk
            cdk destroy "${STACK_NAME}" --force || true
            echo "‚úÖ Stack deleted"
            echo "stack_deleted=true" >> $GITHUB_OUTPUT
          elif [[ "${STACK_STATUS}" == "NOT_FOUND" ]]; then
            echo "Stack does not exist, will create"
            echo "stack_deleted=false" >> $GITHUB_OUTPUT
          else
            echo "‚úÖ Stack exists (${STACK_STATUS})"
            echo "stack_deleted=false" >> $GITHUB_OUTPUT
          fi

          # Note: CDK handles change detection automatically via asset hashing,
          # so we always run cdk deploy and let it determine if updates are needed

      - name: Check if infrastructure changes require deployment
        id: check-deployment
        if: |
          needs.detect-changes.outputs.infrastructure_changed != 'true' &&
          steps.check-stack.outputs.stack_status != 'NOT_FOUND' &&
          inputs.force_deploy != 'true'
        run: |
          echo "No infrastructure file changes detected. Checking CDK diff..."

          cd cdk

          # Run cdk diff to check for actual changes
          if cdk diff "${{ steps.stack-name.outputs.stack_name }}" > /tmp/cdk-diff.txt 2>&1; then
            DIFF_EXIT=$?

            if [ $DIFF_EXIT -eq 0 ]; then
              echo "‚úì CDK diff shows no changes - deployment can be skipped"
              echo "skip_deploy=true" >> $GITHUB_OUTPUT
              echo "skip_reason=No infrastructure changes detected" >> $GITHUB_OUTPUT
            else
              echo "‚ö† CDK diff detected changes despite no file changes (possibly parameter updates)"
              echo "skip_deploy=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "‚ö† CDK diff failed - will deploy to be safe"
            echo "skip_deploy=false" >> $GITHUB_OUTPUT
          fi

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install AWS CDK
        run: npm install -g aws-cdk

      - name: Install CDK dependencies
        run: |
          cd cdk
          pip install -r requirements.txt

      - name: Bootstrap CDK (with recovery)
        if: steps.check-deployment.outputs.skip_deploy != 'true'
        run: |
          echo "Checking CDK bootstrap status..."

          # Try bootstrap first
          if ! cdk bootstrap aws://571930033416/us-east-1 2>&1 | tee bootstrap.log; then
            echo "‚ö†Ô∏è Bootstrap failed, checking for corrupted stack..."

            # Check if error is about existing resources
            if grep -q "already exists" bootstrap.log; then
              echo "Detected corrupted bootstrap stack. Attempting recovery..."

              # Delete the corrupted CDKToolkit stack
              echo "Deleting CDKToolkit stack..."
              aws cloudformation delete-stack --stack-name CDKToolkit || true

              # Wait for deletion
              echo "Waiting for stack deletion..."
              aws cloudformation wait stack-delete-complete --stack-name CDKToolkit || {
                echo "Stack deletion timed out or failed, but continuing anyway..."
              }

              # Re-bootstrap
              echo "Re-bootstrapping CDK environment..."
              cdk bootstrap aws://571930033416/us-east-1
            else
              echo "Bootstrap failed for a different reason. Exiting."
              cat bootstrap.log
              exit 1
            fi
          else
            echo "‚úÖ CDK bootstrap successful"
          fi

      - name: Skip deployment (no changes)
        if: steps.check-deployment.outputs.skip_deploy == 'true'
        run: |
          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
          echo "‚è≠Ô∏è  SKIPPING STACK DEPLOYMENT"
          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
          echo ""
          echo "Reason: ${{ steps.check-deployment.outputs.skip_reason }}"
          echo "Stack: ${{ steps.stack-name.outputs.stack_name }}"
          echo "Status: Stack already exists and is up to date"
          echo ""
          echo "Tests will run against the existing deployed stack."
          echo ""

      - name: Deploy test stack
        if: steps.check-deployment.outputs.skip_deploy != 'true'
        env:
          STACK_NAME: ${{ steps.stack-name.outputs.stack_name }}
          APP_NAME: silvermoat-test
          STAGE_NAME: test
          UI_SEEDING_MODE: external
          CREATE_CLOUDFRONT: false
          DOMAIN_NAME: ""
        run: |
          echo "Deploying test stack: ${STACK_NAME}"
          chmod +x scripts/deploy-stack.sh
          ./scripts/deploy-stack.sh

      - name: Check if UI needs deployment
        id: check-ui
        if: steps.check-deployment.outputs.skip_deploy != 'true'
        run: |
          STACK_NAME="${{ steps.stack-name.outputs.stack_name }}"

          # If stack was just deployed, always deploy UI
          if [ "${{ steps.check-stack.outputs.skip_deploy }}" != "true" ]; then
            echo "Stack was just deployed, UI needs deployment"
            echo "skip_ui=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Get UI bucket from stack outputs
          UI_BUCKET=$(aws cloudformation describe-stacks \
            --stack-name "${STACK_NAME}" \
            --query 'Stacks[0].Outputs[?OutputKey==`UiBucketName`].OutputValue' \
            --output text 2>/dev/null || echo "")

          if [ -z "$UI_BUCKET" ]; then
            echo "‚ö†Ô∏è Could not find UI bucket, will deploy UI"
            echo "skip_ui=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Check if bucket has index.html
          if aws s3 ls "s3://${UI_BUCKET}/index.html" >/dev/null 2>&1; then
            echo "‚úÖ UI already deployed in bucket ${UI_BUCKET}"

            # Calculate hash of UI source to detect changes
            UI_HASH=$(find ui/src ui/public ui/index.html -type f -exec md5sum {} \; 2>/dev/null | sort | md5sum | cut -d' ' -f1)

            # Try to get stored hash from S3
            STORED_HASH=$(aws s3 cp "s3://${UI_BUCKET}/.ui-hash" - 2>/dev/null || echo "")

            if [ "${UI_HASH}" = "${STORED_HASH}" ]; then
              echo "‚úÖ UI source unchanged (hash: ${UI_HASH}), skipping rebuild"
              echo "skip_ui=true" >> $GITHUB_OUTPUT
            else
              echo "UI source changed (${UI_HASH} vs ${STORED_HASH}), will deploy"
              echo "skip_ui=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "UI not found in bucket, will deploy"
            echo "skip_ui=false" >> $GITHUB_OUTPUT
          fi

      - name: Build and deploy React UI
        if: steps.check-deployment.outputs.skip_deploy != 'true' && steps.check-ui.outputs.skip_ui != 'true'
        env:
          STACK_NAME: ${{ steps.stack-name.outputs.stack_name }}
        run: |
          echo "Building and deploying React UI..."
          chmod +x scripts/deploy-ui.sh
          ./scripts/deploy-ui.sh

      - name: Summary of operations
        if: always()
        run: |
          echo "=== Setup Stack Summary ==="
          echo "Stack: ${{ steps.stack-name.outputs.stack_name }}"
          echo "Stack status: ${{ steps.check-stack.outputs.stack_status }}"
          echo "Stack deleted: ${{ steps.check-stack.outputs.stack_deleted }}"
          echo "UI deployment skipped: ${{ steps.check-ui.outputs.skip_ui }}"
          echo ""
          if [ "${{ steps.check-stack.outputs.stack_deleted }}" = "true" ]; then
            echo "üóëÔ∏è Previous failed stack was deleted before redeployment"
          fi
          if [ "${{ steps.check-ui.outputs.skip_ui }}" = "true" ]; then
            echo "‚ö° UI unchanged - saved ~2-3 minutes!"
          fi
          echo ""
          echo "Note: CDK automatically detects and skips unchanged resources"

      - name: Get stack outputs
        id: stack-outputs
        run: |
          STACK_NAME="${{ steps.stack-name.outputs.stack_name }}"

          # Get all outputs as JSON
          OUTPUTS=$(aws cloudformation describe-stacks \
            --stack-name "${STACK_NAME}" \
            --query 'Stacks[0].Outputs' \
            --output json)

          # Extract specific outputs
          API_URL=$(echo "$OUTPUTS" | jq -r '.[] | select(.OutputKey=="ApiBaseUrl") | .OutputValue')

          # Try CloudFrontUrl first (production), fall back to WebUrl (test stacks)
          WEB_URL=$(echo "$OUTPUTS" | jq -r '.[] | select(.OutputKey=="CloudFrontUrl") | .OutputValue')
          if [ -z "$WEB_URL" ] || [ "$WEB_URL" == "null" ]; then
            WEB_URL=$(echo "$OUTPUTS" | jq -r '.[] | select(.OutputKey=="WebUrl") | .OutputValue')
          fi

          echo "api_url=${API_URL}" >> $GITHUB_OUTPUT
          echo "web_url=${WEB_URL}" >> $GITHUB_OUTPUT

          echo "API URL: ${API_URL}"
          echo "Web URL: ${WEB_URL}"

  test-suite:
    needs: setup-stack
    runs-on: ubuntu-latest
    timeout-minutes: 15

    strategy:
      fail-fast: false
      matrix:
        test-type: [smoke, api, e2e]
        include:
          - test-type: smoke
            test-dir: smoke
            needs-chrome: false
          - test-type: api
            test-dir: api
            needs-chrome: false
          - test-type: e2e
            test-dir: e2e
            needs-chrome: true

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup environment
        uses: ./.github/actions/setup-env

      - name: Install Python dependencies
        run: pip install -r requirements-test.txt

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: us-east-1

      - name: Install Chrome and ChromeDriver
        if: matrix.needs-chrome
        uses: browser-actions/setup-chrome@v1
        with:
          chrome-version: stable
          install-chromedriver: true

      - name: Cache ChromeDriver
        if: matrix.needs-chrome
        uses: actions/cache@v4
        with:
          path: /usr/local/bin/chromedriver
          key: ${{ runner.os }}-chromedriver-${{ hashFiles('**/requirements-test.txt') }}
          restore-keys: |
            ${{ runner.os }}-chromedriver-

      - name: Run ${{ matrix.test-type }} tests
        id: test-run
        continue-on-error: true
        env:
          STACK_NAME: ${{ needs.setup-stack.outputs.stack_name }}
          SILVERMOAT_API_URL: ${{ needs.setup-stack.outputs.api_url }}
          SILVERMOAT_URL: ${{ needs.setup-stack.outputs.web_url }}
          HEADLESS: '1'
        run: |
          set -o pipefail
          echo "Running ${{ matrix.test-type }} tests..."
          cd tests/${{ matrix.test-dir }}
          pytest -v --tb=short 2>&1 | tee ../../${{ matrix.test-type }}-tests-output.txt

      - name: Upload test output
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.test-type }}-test-output
          path: ${{ matrix.test-type }}-tests-output.txt
          retention-days: 7

      - name: Upload test artifacts (screenshots, page source, logs)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.test-type }}-test-artifacts
          path: tests/${{ matrix.test-dir }}/test-artifacts/
          retention-days: 7
          if-no-files-found: ignore

      - name: Check test outcome
        if: always()
        run: |
          if [ "${{ steps.test-run.outcome }}" = "failure" ]; then
            echo "${{ matrix.test-type }} tests failed"
            exit 1
          fi

  analyze-results:
    needs: [validate-cdk, setup-stack, test-suite]
    if: always()
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all test outputs
        uses: actions/download-artifact@v4
        with:
          path: test-outputs

      - name: Analyze test results with Claude
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY_GITHUB_ACTIONS }}
        run: |
          echo "Analyzing test results with Claude..."

          # Verify API key is set
          if [ -z "$ANTHROPIC_API_KEY" ]; then
            echo "ERROR: ANTHROPIC_API_KEY_GITHUB_ACTIONS secret is not set"
            echo "API analysis skipped (no API key). Using fallback message." > test-analysis.md
            exit 0
          fi

          # Collect job outcomes
          CFN_OUTCOME="${{ needs.validate-cdk.result }}"
          SETUP_OUTCOME="${{ needs.setup-stack.result }}"
          TESTS_OUTCOME="${{ needs.test-suite.result }}"

          echo "Job outcomes: CFN=$CFN_OUTCOME, SETUP=$SETUP_OUTCOME, TESTS=$TESTS_OUTCOME"

          # Collect test outputs (escape for JSON, limit to 2000 lines each)
          SMOKE_OUTPUT=$(cat test-outputs/smoke-test-output/smoke-tests-output.txt 2>/dev/null | head -2000 | jq -Rs . || echo '"No output captured"')
          API_OUTPUT=$(cat test-outputs/api-test-output/api-tests-output.txt 2>/dev/null | head -2000 | jq -Rs . || echo '"No output captured"')
          E2E_OUTPUT=$(cat test-outputs/e2e-test-output/e2e-tests-output.txt 2>/dev/null | head -2000 | jq -Rs . || echo '"No output captured"')

          # Collect debug artifacts (concise summaries for failed tests)
          E2E_ARTIFACTS=""
          if [ -d "test-outputs/e2e-test-artifacts" ]; then
            echo "Collecting E2E test artifacts..."
            for artifact in test-outputs/e2e-test-artifacts/*.txt; do
              if [ -f "$artifact" ]; then
                E2E_ARTIFACTS="${E2E_ARTIFACTS}\n\n### $(basename $artifact)\n$(cat $artifact | head -50)"
              fi
            done
          fi
          E2E_ARTIFACTS_JSON=$(echo "$E2E_ARTIFACTS" | jq -Rs . || echo '""')

          # Determine outcomes from job results
          SMOKE_OUTCOME="${{ needs.test-suite.result }}"
          API_OUTCOME="${{ needs.test-suite.result }}"
          E2E_OUTCOME="${{ needs.test-suite.result }}"

          # Build JSON request with proper escaping
          jq -n \
            --arg cfn_outcome "$CFN_OUTCOME" \
            --arg setup_outcome "$SETUP_OUTCOME" \
            --arg tests_outcome "$TESTS_OUTCOME" \
            --arg smoke_outcome "$SMOKE_OUTCOME" \
            --arg api_outcome "$API_OUTCOME" \
            --arg e2e_outcome "$E2E_OUTCOME" \
            --argjson smoke_output "$SMOKE_OUTPUT" \
            --argjson api_output "$API_OUTPUT" \
            --argjson e2e_output "$E2E_OUTPUT" \
            --argjson e2e_artifacts "$E2E_ARTIFACTS_JSON" \
            '{
              "model": "claude-sonnet-4-5",
              "max_tokens": 4096,
              "messages": [{
                "role": "user",
                "content": "You are analyzing GitHub Actions workflow results for the Silvermoat insurance MVP application. Review ALL job outcomes and outputs below, then generate a concise PR comment summarizing:\n\n1. Overall pipeline status (‚úÖ passed / ‚ö†Ô∏è partial / ‚ùå failed)\n2. Key findings for EACH job (CloudFormation validation, stack deployment, tests)\n3. Root cause analysis for any failures (check CFN validation and stack deployment FIRST)\n4. Critical failures that need immediate attention\n5. Recommendations for fixes\n\nFormat as GitHub-flavored markdown. Be specific about which jobs/tests failed and why. Limit to 500 words.\n\n## Pipeline Job Outcomes\n- CloudFormation Validation: \($cfn_outcome)\n- Stack Deployment: \($setup_outcome)\n- Test Suite: \($tests_outcome)\n\n## Test Suite Outcomes (if tests ran)\n- Smoke tests: \($smoke_outcome)\n- API tests: \($api_outcome)\n- E2E tests: \($e2e_outcome)\n\n## Test Outputs\n\n### Deployment Smoke Tests\n```\n\($smoke_output)\n```\n\n### API Contract Tests\n```\n\($api_output)\n```\n\n### E2E Smoke Tests\n```\n\($e2e_output)\n```\n\n### E2E Debug Artifacts\nThese contain DOM state, console errors, and diagnostic info for failed tests:\n```\n\($e2e_artifacts)\n```\n\n## Analysis Instructions\n\n**IMPORTANT**: If CFN validation or stack deployment failed, focus on that as the root cause. Tests cannot run without a deployed stack. Check for:\n- CFN template syntax errors (cfn-lint output)\n- Stack deployment failures (missing resources, circular dependencies, parameter issues)\n- Rollback states (ROLLBACK_COMPLETE, ROLLBACK_FAILED)\n\nOnly analyze test failures if the stack deployed successfully."
              }]
            }' > analysis-request.json

          # Call Claude API (60s timeout for analysis)
          set +e  # Don't exit on curl error
          curl -s --max-time 60 -o analysis-response.json -w "%{http_code}" \
            https://api.anthropic.com/v1/messages \
            -H "content-type: application/json" \
            -H "x-api-key: $ANTHROPIC_API_KEY" \
            -H "anthropic-version: 2023-06-01" \
            -d @analysis-request.json > http_code.txt
          CURL_EXIT=$?
          HTTP_CODE=$(cat http_code.txt 2>/dev/null || echo "000")
          set -e  # Re-enable exit on error

          echo "Curl exit code: $CURL_EXIT"
          echo "API response code: $HTTP_CODE"

          # Check for curl failures
          if [ $CURL_EXIT -ne 0 ]; then
            echo "Curl command failed with exit code $CURL_EXIT"
            if [ $CURL_EXIT -eq 28 ]; then
              echo "Timeout after 60 seconds"
            elif [ $CURL_EXIT -eq 6 ]; then
              echo "Could not resolve host api.anthropic.com"
            else
              echo "Connection or network error"
            fi
            echo "API analysis failed (connection error). Using fallback message." > test-analysis.md
            exit 0
          fi

          # Check HTTP response codes
          if [ "$HTTP_CODE" = "401" ]; then
            echo "Authentication failed - check ANTHROPIC_API_KEY secret"
            cat analysis-response.json 2>/dev/null || echo "No response body"
            echo "API analysis failed (authentication). Using fallback message." > test-analysis.md
            exit 0
          elif [ "$HTTP_CODE" = "429" ]; then
            echo "Rate limit exceeded"
            cat analysis-response.json 2>/dev/null || echo "No response body"
            echo "API analysis failed (rate limit). Using fallback message." > test-analysis.md
            exit 0
          elif [ "$HTTP_CODE" != "200" ]; then
            echo "API request failed with HTTP code $HTTP_CODE"
            cat analysis-response.json 2>/dev/null || echo "No response body"
            echo "API analysis failed (HTTP $HTTP_CODE). Using fallback message." > test-analysis.md
            exit 0
          fi

          # Extract the analysis
          ANALYSIS=$(jq -r '.content[0].text // "Analysis extraction failed"' analysis-response.json)

          # Save for next step
          echo "$ANALYSIS" > test-analysis.md
          echo "Analysis complete!"

      - name: Extract issues from test failures
        if: always() && (inputs.create_issues == true || github.event_name == 'pull_request')
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY_GITHUB_ACTIONS }}
        run: |
          echo "Extracting actionable issues from test failures..."

          # Skip if no API key or analysis failed
          if [ -z "$ANTHROPIC_API_KEY" ] || [ ! -f "test-analysis.md" ]; then
            echo "[]" > issues-to-create.json
            exit 0
          fi

          # Collect test outputs again
          SMOKE_OUTPUT=$(cat test-outputs/smoke-test-output/smoke-tests-output.txt 2>/dev/null | head -1000 | jq -Rs . || echo '"No output"')
          API_OUTPUT=$(cat test-outputs/api-test-output/api-tests-output.txt 2>/dev/null | head -1000 | jq -Rs . || echo '"No output"')
          E2E_OUTPUT=$(cat test-outputs/e2e-test-output/e2e-tests-output.txt 2>/dev/null | head -1000 | jq -Rs . || echo '"No output"')

          # Collect artifacts for better issue context
          E2E_ARTIFACTS=""
          if [ -d "test-outputs/e2e-test-artifacts" ]; then
            for artifact in test-outputs/e2e-test-artifacts/*.txt; do
              if [ -f "$artifact" ]; then
                E2E_ARTIFACTS="${E2E_ARTIFACTS}\n\n### $(basename $artifact)\n$(cat $artifact | head -50)"
              fi
            done
          fi
          E2E_ARTIFACTS_JSON=$(echo "$E2E_ARTIFACTS" | jq -Rs . || echo '""')

          # Ask Claude to extract discrete issues
          jq -n \
            --argjson smoke "$SMOKE_OUTPUT" \
            --argjson api "$API_OUTPUT" \
            --argjson e2e "$E2E_OUTPUT" \
            --argjson e2e_artifacts "$E2E_ARTIFACTS_JSON" \
            '{
              "model": "claude-sonnet-4-5",
              "max_tokens": 2048,
              "messages": [{
                "role": "user",
                "content": "Analyze these test failures and extract discrete, actionable issues. For each distinct problem, provide:\n\n1. Title (concise, specific to the failure)\n2. Body (details: what failed, error messages, reproduction context, DOM state from artifacts)\n3. Labels (array: e.g., [\"bug\", \"tests\", \"e2e\"])\n\nReturn ONLY a JSON array (no markdown, no explanation):\n```json\n[\n  {\n    \"title\": \"E2E test timeout: Sample data button not found\",\n    \"body\": \"## Problem\\n[details]\\n\\n## Error\\n```\\n[error]\\n```\\n\\n## DOM State\\n[from artifacts]\\n\\n## Context\\nWorkflow: ...\\n\",\n    \"labels\": [\"bug\", \"e2e-tests\"]\n  }\n]\n```\n\nIf no distinct issues (all tests pass, or failures are duplicates), return: []\n\n## Smoke Tests\n```\n\($smoke)\n```\n\n## API Tests\n```\n\($api)\n```\n\n## E2E Tests\n```\n\($e2e)\n```\n\n## E2E Debug Artifacts (DOM state, console errors)\n```\n\($e2e_artifacts)\n```"
              }]
            }' > issues-request.json

          # Call Claude API
          set +e
          curl -s --max-time 45 -o issues-response.json -w "%{http_code}" \
            https://api.anthropic.com/v1/messages \
            -H "content-type: application/json" \
            -H "x-api-key: $ANTHROPIC_API_KEY" \
            -H "anthropic-version: 2023-06-01" \
            -d @issues-request.json > issues_http_code.txt
          CURL_EXIT=$?
          HTTP_CODE=$(cat issues_http_code.txt 2>/dev/null || echo "000")
          set -e

          # Handle failures
          if [ $CURL_EXIT -ne 0 ] || [ "$HTTP_CODE" != "200" ]; then
            echo "Issue extraction failed (curl: $CURL_EXIT, http: $HTTP_CODE)"
            echo "[]" > issues-to-create.json
            exit 0
          fi

          # Extract JSON array from response
          ISSUES_JSON=$(jq -r '.content[0].text // "[]"' issues-response.json)

          # Try to parse it - Claude might wrap in markdown code blocks
          if echo "$ISSUES_JSON" | grep -q '```json'; then
            # Extract JSON from markdown code block
            ISSUES_JSON=$(echo "$ISSUES_JSON" | sed -n '/```json/,/```/p' | sed '1d;$d')
          elif echo "$ISSUES_JSON" | grep -q '```'; then
            ISSUES_JSON=$(echo "$ISSUES_JSON" | sed -n '/```/,/```/p' | sed '1d;$d')
          fi

          # Validate JSON
          if echo "$ISSUES_JSON" | jq empty 2>/dev/null; then
            echo "$ISSUES_JSON" > issues-to-create.json
            ISSUE_COUNT=$(echo "$ISSUES_JSON" | jq 'length')
            echo "Extracted $ISSUE_COUNT issue(s) to create"
          else
            echo "Failed to parse issues JSON, skipping issue creation"
            echo "[]" > issues-to-create.json
          fi

      - name: Upload analysis and issues
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-analysis
          path: |
            test-analysis.md
            issues-to-create.json
          retention-days: 7

  post-to-pr:
    needs: [setup-stack, analyze-results]
    if: always() && github.event_name == 'pull_request'
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
      - name: Download analysis
        uses: actions/download-artifact@v4
        with:
          name: test-analysis

      - name: Create GitHub issues for test failures
        id: create-issues
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            // Read issues to create
            let issuesToCreate = [];
            try {
              const issuesJson = fs.readFileSync('issues-to-create.json', 'utf8');
              issuesToCreate = JSON.parse(issuesJson);
            } catch (error) {
              console.log('No issues to create or failed to read:', error);
              core.setOutput('created_issues', JSON.stringify([]));
              return;
            }

            if (!Array.isArray(issuesToCreate) || issuesToCreate.length === 0) {
              console.log('No issues to create');
              core.setOutput('created_issues', JSON.stringify([]));
              return;
            }

            console.log(`Creating ${issuesToCreate.length} issue(s)...`);

            const createdIssues = [];
            const runUrl = `${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
            const prNumber = context.payload.pull_request.number;

            for (const issue of issuesToCreate) {
              try {
                // Add workflow context to issue body
                const bodyWithContext = `${issue.body}\n\n---\n\n**Detected by:** Workflow run [#${context.runId}](${runUrl})\n**Related PR:** #${prNumber}`;

                const result = await github.rest.issues.create({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  title: issue.title,
                  body: bodyWithContext,
                  labels: issue.labels || ['bug', 'automated']
                });

                createdIssues.push({
                  number: result.data.number,
                  title: result.data.title,
                  url: result.data.html_url
                });

                console.log(`Created issue #${result.data.number}: ${result.data.title}`);
              } catch (error) {
                console.error(`Failed to create issue "${issue.title}":`, error);
              }
            }

            core.setOutput('created_issues', JSON.stringify(createdIssues));

      - name: Post results to PR
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const prNumber = context.payload.pull_request.number;
            const runUrl = `${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
            const stackName = '${{ needs.setup-stack.outputs.stack_name }}' || 'unknown';

            // Read AI-generated analysis
            let analysis = "Test analysis not available";
            try {
              analysis = fs.readFileSync('test-analysis.md', 'utf8');
            } catch (error) {
              console.log('Could not read analysis file:', error);
              analysis = "‚ö†Ô∏è AI analysis failed. Check logs for details.";
            }

            // Check if deployment was skipped
            const deploySkipped = '${{ needs.setup-stack.outputs.skip_deploy }}' === 'true';
            const skipReason = '${{ needs.setup-stack.outputs.skip_reason }}' || '';
            let deploySkippedNote = '';
            if (deploySkipped) {
              deploySkippedNote = `\n  ‚è≠Ô∏è **Deployment Skipped** (${skipReason})`;
            }

            // Read created issues from previous step
            const createdIssuesJson = '${{ steps.create-issues.outputs.created_issues }}';
            let createdIssues = [];
            try {
              createdIssues = JSON.parse(createdIssuesJson);
            } catch (error) {
              console.log('No created issues:', error);
            }

            // Build issues section if any were created
            let issuesSection = '';
            if (createdIssues.length > 0) {
              issuesSection = '\n\n## üìã Issues Created\n\n';
              for (const issue of createdIssues) {
                issuesSection += `- #${issue.number} ${issue.title}\n`;
              }
            }

            // Fetch per-test-type results from matrix jobs
            let smokeResult = 'unknown';
            let apiResult = 'unknown';
            let e2eResult = 'unknown';

            try {
              const { data: workflowJobs } = await github.rest.actions.listJobsForWorkflowRun({
                owner: context.repo.owner,
                repo: context.repo.repo,
                run_id: context.runId
              });

              // Find test-suite matrix jobs and extract results
              for (const job of workflowJobs.jobs) {
                if (job.name.includes('test-suite (smoke)')) {
                  smokeResult = job.conclusion || job.status;
                } else if (job.name.includes('test-suite (api)')) {
                  apiResult = job.conclusion || job.status;
                } else if (job.name.includes('test-suite (e2e)')) {
                  e2eResult = job.conclusion || job.status;
                }
              }
            } catch (error) {
              console.log('Failed to fetch test-suite results:', error);
            }

            // Build stack status section
            const cfnOutcome = '${{ needs.analyze-results.result }}';
            const setupOutcome = '${{ needs.setup-stack.result }}';
            const testsOutcome = '${{ needs.analyze-results.result }}';

            let statusEmoji = '‚úÖ';
            if (cfnOutcome === 'failure' || setupOutcome === 'failure') {
              statusEmoji = '‚ùå';
            } else if (testsOutcome === 'failure') {
              statusEmoji = '‚ö†Ô∏è';
            }

            // Helper function for result emoji
            const getEmoji = (result) => {
              if (result === 'success') return '‚úÖ';
              if (result === 'failure') return '‚ùå';
              if (result === 'skipped' || result === 'cancelled') return '‚è≠Ô∏è';
              return '‚ùì';
            };

            const body = `## ${statusEmoji} Pipeline Results

            **Test Stack:** \`${stackName}\`
            **Run:** [View Full Logs](${runUrl})

            **Job Results:**
            - CloudFormation Validation: ${cfnOutcome === 'success' ? '‚úÖ' : '‚ùå'} \`${cfnOutcome}\`
            - Stack Deployment: ${setupOutcome === 'success' ? '‚úÖ' : '‚ùå'} \`${setupOutcome}\`${deploySkippedNote}
            - Test Suite: ${testsOutcome === 'success' ? '‚úÖ' : testsOutcome === 'skipped' ? '‚è≠Ô∏è' : '‚ùå'} \`${testsOutcome}\`

            **Test Results:**
            - Smoke Tests: ${getEmoji(smokeResult)} \`${smokeResult}\`
            - API Tests: ${getEmoji(apiResult)} \`${apiResult}\`
            - E2E Tests: ${getEmoji(e2eResult)} \`${e2eResult}\`

            ---

            ${analysis}${issuesSection}

            ---

            <sub>Stack \`${stackName}\` ${setupOutcome === 'success' ? 'is running and will be cleaned up when this PR is closed' : 'may not have deployed successfully - check logs above'}. Analysis generated by Claude Sonnet 4.</sub>
            `;

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: prNumber,
              body: body
            });

  check-results:
    needs: test-suite
    if: always()
    runs-on: ubuntu-latest
    timeout-minutes: 2

    steps:
      - name: Check test results
        run: |
          echo "Checking test suite results..."
          echo "Test suite job result: ${{ needs.test-suite.result }}"

          if [ "${{ needs.test-suite.result }}" = "failure" ]; then
            echo "One or more test suites failed"
            exit 1
          fi
          echo "All test suites passed"
